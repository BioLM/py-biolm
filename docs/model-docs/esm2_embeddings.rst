..
   Copyright (c) 2021 Pradyun Gedam
   Licensed under Creative Commons Attribution-ShareAlike 4.0 International License
   SPDX-License-Identifier: CC-BY-SA-4.0


=========
ESM2 Embeddings
=========

.. article-info::
    :avatar: img/book_icon.png
    :date: Oct 18, 2021
    :read-time: 6 min read
    :author: Zeeshan Siddiqui
    :class-container: sd-p-2 sd-outline-muted sd-rounded-1

*On this page, we will show and explain the use of ESM2 for generating embeddings. Document the BioLM API for tokenization, demonstrate no-code and code interfaces to protein embeddings/tokenization.*

-----------
Description
-----------

One of the best ways you can leverage a large language model is for feature generation. 
The internal, numeric representations the neural-net uses to make predictions can be output and used for downstream machine learning tasks. 
The numeric vectors from NLP models often encode additional, powerful information beyond simple one-hot encodings. Usually feature engineering for biology is heavily task-specific, but in this case the embeddings can be used for a variety of classification, regression, and other tasks. 
The BioLM API democratizes protein analysis by providing easy access to ESM-2 for generating insightful protein embeddings. 
This service accelerates tasks from sequence similarity detection to therapeutic antibody design, simplifying the transition from protein sequence data to actionable insights.


--------
Benefits
--------

* The API can be used by biologists, data scientists, engineers, etc. The key values of the BioLM API is speed, scalability and cost.

* The BioLM API allows scientists to programmatically interact with ESM-1V, making it easier to integrate the model into their scientific workflows. The API accelerates workflow, allows for customization, and is designed to be highly scalable.

* Our unique API UI Chat allows users to interact with our API and access multiple language models without the need to code!

* The benefit of having access to multiple GPUs is parallel processing.

---------
Performance
---------

Graph of average RPS for varying number of sequences (ESM-2 Embeddings)

.. figure:: 
   :scale: 
   :alt: 

   This is the caption of the figure (a simple paragraph).

   The legend consists of all elements after the caption.

.. note::
   This graph will be added very soon.  



---------
API Usage
---------

This is the url to use when querying the BioLM ESM-1V Prediction Endpoint: https://biolm.ai/api/v1/models/esm2_t33_650M_UR50D/transform/


*Definitions*

-Request Keys:

data: 
   Inside each instance, there's a key named "data" that holds another dictionary. This dictionary contains the actual input data for the prediction.

text: 
   Inside the "data" dictionary, there's a key named "text". The value associated with "text" should be a string containing the amino acid sequence that the user wants to submit for structure prediction.

-Response Keys:

predictions: 
   This is the main key in the JSON object that contains an array of prediction results. Each element in the array represents a set of predictions for one input instance.

mean_representations: 
   This key holds the embeddings generated by the ESM-2 model for the corresponding input instance. These embeddings represent average values computed over certain dimensions of the model's output. 

'33': 
   Specifying a particular layer or dimension of the model's output from which the embeddings were derived.



^^^^^^^^^^^^^^^
Making Requests
^^^^^^^^^^^^^^^

.. tab-set::

    .. tab-item:: Curl
        :sync: curl

        .. code:: shell

            curl --location 'https://biolm.ai/api/v1/models/esm2_t33_650M_UR50D/predict/' \
            --header "Authorization: Token ed3fa24ec0432c5ba812a66d7b8931914c73a208d287af387b97bb3ee4cf907e" \
            --header 'Content-Type: application/json' \
            --data '{
            "instances": [{
               "data": {"text": "MSILVTRPSPAGEELVSRLRTLGQVAWHFPLIEFSPGQQLPQLADQLAALGESDLLFALSQHAVAFAQSQLHQQDRKWPRLPDYFAIGRTTALALHTVSGQKILYPQDREISEVLLQLPELQNIAGKRALILRGNGGRELIGDTLTARGAEVTFCECYQRCAIHYDGAEEAMRWQAREVTMVVVTSGEMLQQLWSLIPQWYREHWLLHCRLLVVSERLAKLARELGWQDIKVADNADNDALLRALQ"}
            }]
            }'


    .. tab-item:: Python Requests
        :sync: python

        .. code:: python

            import requests
            import json

            url = "https://biolm.ai/api/v1/models/esm2_t33_650M_UR50D/predict/"

            payload = json.dumps({
            "instances": [
               {
                  "data": {
                  "text": "MSILVTRPSPAGEELVSRLRTLGQVAWHFPLIEFSPGQQLPQLADQLAALGESDLLFALSQHAVAFAQSQLHQQDRKWPRLPDYFAIGRTTALALHTVSGQKILYPQDREISEVLLQLPELQNIAGKRALILRGNGGRELIGDTLTARGAEVTFCECYQRCAIHYDGAEEAMRWQAREVTMVVVTSGEMLQQLWSLIPQWYREHWLLHCRLLVVSERLAKLARELGWQDIKVADNADNDALLRALQ"
                  }
               }
            ]
            })
            headers = {
            'Authorization': 'Token {}'.format(os.environ['ed3fa24ec0432c5ba812a66d7b8931914c73a208d287af387b97bb3ee4cf907e']),
            'Content-Type': 'application/json'
            }

            response = requests.request("POST", url, headers=headers, data=payload)

            print(response.text)

    .. tab-item:: biolmai SDK
        :sync: sdk

        Content 2

    .. tab-item:: R
        :sync: r

        .. code:: R

            library(RCurl)
            headers = c(
            'Authorization' = paste('Token', Sys.getenv('BIOLMAI_TOKEN')),
            "Content-Type" = "application/json"
            )
            params = "{
            \"instances\": [
               {
                  \"data\": {
                  \"text\": \"MSILVTRPSPAGEELVSRLRTLGQVAWHFPLIEFSPGQQLPQLADQLAALGESDLLFALSQHAVAFAQSQLHQQDRKWPRLPDYFAIGRTTALALHTVSGQKILYPQDREISEVLLQLPELQNIAGKRALILRGNGGRELIGDTLTARGAEVTFCECYQRCAIHYDGAEEAMRWQAREVTMVVVTSGEMLQQLWSLIPQWYREHWLLHCRLLVVSERLAKLARELGWQDIKVADNADNDALLRALQ\"
                  }
               }
            ]
            }"
            res <- postForm("https://biolm.ai/api/v1/models/esm2_t33_650M_UR50D/predict/", .opts=list(postfields = params, httpheader = headers, followlocation = TRUE), style = "httppost")
            cat(res)


^^^^^^^^^^^^^
JSON Response
^^^^^^^^^^^^^

.. dropdown:: Expand Example Response

    .. code:: json

         {
         "predictions": [
            {
               "name": "0",
               "mean_representations": {
               "33": [
                  0.008923606015741825,
                  -0.005895234644412994,
                  -0.0060966904275119305,
                  -0.016010720282793045,
                  -0.14031203091144562,
                  -0.044720884412527084,

   .. note:: 
      The above response is only a small snippet of the full JSON response. However, all the relevant response keys are included. 
   


----------
Related 
----------
* ESMFold (singlechain): :ref:`docs/model-docs/esm2_fold.rst` 

* ESM-1V (Masking): :ref:`docs/model-docs/esm_1v_masking.rst`


------------------
Model Background
------------------

The SOTA transformer protein language model, ESM-2, leverages much more data, a vastly larger model, and enhanced training methodology to generate superior contextual embeddings compared to its ESM-1v predecessor across a variety of protein modeling applications.

“The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models” *-Lin et al., 2022*

ESM-2 was pretrained on a much larger dataset, incorporating all sequences from UniRef50 rather than just a subset. This amounted to 200 million sequences comprising 120 billion amino acid residues. In addition, ESM-2 utilizes a drastically larger model architecture, with 33 transformer layers and 1.6 billion parameters, dwarfing ESM-1v's 12 layers and 128 million parameters.

For ESM-2 training, a multi-node network was utilized to tackle the communication bottleneck observed as models grow in size. This approach leveraged the propensity of BERT-style models for large batch sizes, raising the effective batch size to 2 million tokens, where each token represents the smallest learnable unit of data, akin to an individual amino acid or a small amino acid sequence in protein sequences. For the 15 billion parameter model, the batch size was further increased to 3.2 million tokens to accelerate the training process in each iteration. The model architecture transitioned to using Sparsely-Gated Mixture-of-Experts instead of standard transformers. The pre-training phase encompassed joint language modeling and supervised auxiliary losses, capturing a broader spectrum of information beyond just language modeling. These enhancements in ESM-2 significantly improve sequence representations, especially for long protein sequences, propelling it to a state-of-the-art (SOTA) standing across various downstream prediction tasks.




-----------------------
Applications of ESM-2
-----------------------

Protein embeddings from ESM-2 can be instrumental in predicting protein-protein interactions, aiding in designing proteins with specified interaction capabilities. 
They also facilitate function prediction of uncharacterized or newly discovered proteins, thus broadening the understanding of protein functionalities. 
Additionally, these embeddings can be utilized to anticipate the effects of mutations on protein function and stability, which is pivotal in protein design. 
In the realm of drug discovery, embeddings can assist in identifying potential new drug targets by analyzing structural or functional similarities with known targets. Lastly, they can expedite comparative analysis by providing a high-dimensional representation of protein sequences, aiding in the identification of conserved domains or regions of interest, which is crucial for understanding evolutionary relationships and functional conservations among proteins.


* Enzyme engineering (enzyme optimization, transfer learning, directed evolution).

* Antibody engineering (Machine learning models applied on antibody embeddings may predict affinity, expression, stability without lab assays). 

* Protein-protein interaction design - Embeddings can be used to engineer proteins that interact with specific targets, like designing cellular signaling proteins.

* Membrane protein design.









